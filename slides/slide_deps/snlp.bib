
@misc{mozes_use_2023,
	title = {Use of {LLMs} for {Illicit} {Purposes}: {Threats}, {Prevention} {Measures}, and {Vulnerabilities}},
	shorttitle = {Use of {LLMs} for {Illicit} {Purposes}},
	url = {http://arxiv.org/abs/2308.12833},
	abstract = {Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures. With our work, we hope to raise awareness of the limitations of LLMs in light of such security concerns, among both experienced developers and novel users of such technologies.},
	urldate = {2023-08-28},
	publisher = {arXiv},
	author = {Mozes, Maximilian and He, Xuanli and Kleinberg, Bennett and Griffin, Lewis D.},
	month = aug,
	year = {2023},
	note = {arXiv:2308.12833 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/4WRWP8WA/2308.html:text/html},
}



@article{gentzkow_text_2019,
	title = {Text as {Data}},
	volume = {57},
	issn = {0022-0515},
	url = {https://pubs.aeaweb.org/doi/10.1257/jel.20181020},
	doi = {10.1257/jel.20181020},
	language = {en},
	number = {3},
	urldate = {2020-04-04},
	journal = {Journal of Economic Literature},
	author = {Gentzkow, Matthew and Kelly, Bryan and Taddy, Matt},
	month = sep,
	year = {2019},
	pages = {535--574},
}

@incollection{aref_women_2020,
	address = {Cham},
	title = {Women {Worry} {About} {Family}, {Men} {About} the {Economy}: {Gender} {Differences} in {Emotional} {Responses} to {COVID}-19},
	volume = {12467},
	isbn = {978-3-030-60974-0 978-3-030-60975-7},
	shorttitle = {Women {Worry} {About} {Family}, {Men} {About} the {Economy}},
	url = {http://link.springer.com/10.1007/978-3-030-60975-7_29},
	language = {en},
	urldate = {2020-10-25},
	booktitle = {Social {Informatics}},
	publisher = {Springer},
	author = {van der Vegt, Isabelle and Kleinberg, Bennett},
	editor = {Aref, Samin and Bontcheva, Kalina and Braghieri, Marco and Dignum, Frank and Giannotti, Fosca and Grisolia, Francesco and Pedreschi, Dino},
	year = {2020},
	doi = {10.1007/978-3-030-60975-7_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {397--409},
}

@inproceedings{kleinberg_measuring_2020,
	address = {Online},
	title = {Measuring {Emotions} in the {COVID}-19 {Real} {World} {Worry} {Dataset}},
	url = {https://www.aclweb.org/anthology/2020.nlpcovid19-acl.11},
	abstract = {The COVID-19 pandemic is having a dramatic impact on societies and economies around the world. With various measures of lockdowns and social distancing in place, it becomes important to understand emotional responses on a large scale. In this paper, we present the first ground truth dataset of emotional responses to COVID-19. We asked participants to indicate their emotions and express these in text. This resulted in the Real World Worry Dataset of 5,000 texts (2,500 short + 2,500 long texts). Our analyses suggest that emotional responses correlated with linguistic measures. Topic modeling further revealed that people in the UK worry about their family and the economic situation. Tweet-sized texts functioned as a call for solidarity, while longer texts shed light on worries and concerns. Using predictive modeling approaches, we were able to approximate the emotional responses of participants from text within 14\% of their actual value. We encourage others to use the dataset and improve how we can use automated methods to learn about emotional responses and worries about an urgent problem.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 1st {Workshop} on {NLP} for {COVID}-19 at {ACL} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Kleinberg, Bennett and van der Vegt, Isabelle and Mozes, Maximilian},
	month = jul,
	year = {2020},
}

@book{salganik_bit_2019,
	address = {Princeton, NJ},
	title = {Bit by bit: {Social} research in the digital age},
	isbn = {978-0-691-19610-7},
	shorttitle = {Bit by bit},
	language = {English},
	publisher = {Princeton University Press},
	author = {Salganik, Matthew J},
	year = {2019},
	note = {OCLC: 1134658838},
}

@article{boyd_natural_2021,
	title = {Natural {Language} {Analysis} and the {Psychology} of {Verbal} {Behavior}: {The} {Past}, {Present}, and {Future} {States} of the {Field}},
	volume = {40},
	issn = {0261-927X},
	shorttitle = {Natural {Language} {Analysis} and the {Psychology} of {Verbal} {Behavior}},
	url = {https://doi.org/10.1177/0261927X20967028},
	doi = {10.1177/0261927X20967028},
	abstract = {Throughout history, scholars and laypeople alike have believed that our words contain subtle clues about what we are like as people, psychologically speaking. However, the ways in which language has been used to infer psychological processes has seen dramatic shifts over time and, with modern computational technologies and digital data sources, we are on the verge of a massive revolution in language analysis research. In this article, we discuss the past and current states of research at the intersection of language analysis and psychology, summarizing the central successes and shortcomings of psychological text analysis to date. We additionally outline and discuss a critical need for language analysis practitioners in the social sciences to expand their view of verbal behavior. Lastly, we discuss the trajectory of interdisciplinary research on language and the challenges of integrating analysis methods across paradigms, recommending promising future directions for the field along the way.},
	language = {en},
	number = {1},
	urldate = {2021-08-18},
	journal = {Journal of Language and Social Psychology},
	author = {Boyd, Ryan L. and Schwartz, H. Andrew},
	month = jan,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	keywords = {attention, computational social science, language analysis, natural language processing},
	pages = {21--41},
}

@article{mozes_repeated-measures_2021,
	title = {A repeated-measures study on emotional responses after a year in the pandemic},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-02414-9},
	doi = {10.1038/s41598-021-02414-9},
	abstract = {The introduction of COVID-19 lockdown measures and an outlook on return to normality are demanding societal changes. Among the most pressing questions is how individuals adjust to the pandemic. This paper examines the emotional responses to the pandemic in a repeated-measures design. Data (nâ€‰=â€‰1698) were collected in April 2020 (during strict lockdown measures) and in April 2021 (when vaccination programmes gained traction). We asked participants to report their emotions and express these in text data. Statistical tests revealed an average trend towards better adjustment to the pandemic. However, clustering analyses suggested a more complex heterogeneous pattern with a well-coping and a resigning subgroup of participants. Linguistic computational analyses uncovered that topics and n-gram frequencies shifted towards attention to the vaccination programme and away from general worrying. Implications for public mental health efforts in identifying people at heightened risk are discussed. The dataset is made publicly available.},
	language = {en},
	number = {1},
	urldate = {2021-12-03},
	journal = {Scientific Reports},
	author = {Mozes, Maximilian and van der Vegt, Isabelle and Kleinberg, Bennett},
	month = nov,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Human behaviour;Population screening
Subject\_term\_id: human-behaviour;population-screening},
	keywords = {Human behaviour, Population screening},
	pages = {23114},
	file = {Snapshot:/Users/bennettkleinberg/Zotero/storage/6B6EY4IH/s41598-021-02414-9.html:text/html},
}

@inproceedings{morris-etal-2020-textattack,
    title = "{T}ext{A}ttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in {NLP}",
    author = "Morris, John  and
      Lifland, Eli  and
      Yoo, Jin Yong  and
      Grigsby, Jake  and
      Jin, Di  and
      Qi, Yanjun",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.16",
    doi = "10.18653/v1/2020.emnlp-demos.16",
    pages = "119--126",
    abstract = "While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack{'}s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.",
}

@inproceedings{mozes-etal-2021-contrasting,
    title = "Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification",
    author = "Mozes, Maximilian  and
      Bartolo, Max  and
      Stenetorp, Pontus  and
      Kleinberg, Bennett  and
      Griffin, Lewis",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.651",
    doi = "10.18653/v1/2021.emnlp-main.651",
    pages = "8258--8270",
    abstract = "Research shows that natural language processing models are generally considered to be vulnerable to adversarial attacks; but recent work has drawn attention to the issue of validating these adversarial inputs against certain criteria (e.g., the preservation of semantics and grammaticality). Enforcing constraints to uphold such criteria may render attacks unsuccessful, raising the question of whether valid attacks are actually feasible. In this work, we investigate this through the lens of human language ability. We report on crowdsourcing studies in which we task humans with iteratively modifying words in an input text, while receiving immediate model feedback, with the aim of causing a sentiment classification model to misclassify the example. Our findings suggest that humans are capable of generating a substantial amount of adversarial examples using semantics-preserving word substitutions. We analyze how human-generated adversarial examples compare to the recently proposed TextFooler, Genetic, BAE and SememePSO attack algorithms on the dimensions naturalness, preservation of sentiment, grammaticality and substitution rate. Our findings suggest that human-generated adversarial examples are not more able than the best algorithms to generate natural-reading, sentiment-preserving examples, though they do so by being much more computationally efficient.",
}

@inproceedings{mozes-etal-2021-frequency,
    title = "Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples",
    author = "Mozes, Maximilian  and
      Stenetorp, Pontus  and
      Kleinberg, Bennett  and
      Griffin, Lewis",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.13",
    doi = "10.18653/v1/2021.eacl-main.13",
    pages = "171--186",
    abstract = "Recent efforts have shown that neural text processing models are vulnerable to adversarial examples, but the nature of these examples is poorly understood. In this work, we show that adversarial attacks against CNN, LSTM and Transformer-based classification models perform word substitutions that are identifiable through frequency differences between replaced words and their corresponding substitutions. Based on these findings, we propose frequency-guided word substitutions (FGWS), a simple algorithm exploiting the frequency properties of adversarial word substitutions for the detection of adversarial examples. FGWS achieves strong performance by accurately detecting adversarial examples on the SST-2 and IMDb sentiment datasets, with F1 detection scores of up to 91.4{\%} against RoBERTa-based classification models. We compare our approach against a recently proposed perturbation discrimination framework and show that we outperform it by up to 13.0{\%} F1.",
}

@inproceedings{alzantot-etal-2018-generating,
    title = "Generating Natural Language Adversarial Examples",
    author = "Alzantot, Moustafa  and
      Sharma, Yash  and
      Elgohary, Ahmed  and
      Ho, Bo-Jhang  and
      Srivastava, Mani  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1316",
    doi = "10.18653/v1/D18-1316",
    pages = "2890--2896",
    abstract = "Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97{\%} and 70{\%}, respectively. We additionally demonstrate that 92.3{\%} of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.",
}

@article{hofman2021integrating,
  title={Integrating explanation and prediction in computational social science},
  author={Hofman, Jake M and Watts, Duncan J and Athey, Susan and Garip, Filiz and Griffiths, Thomas L and Kleinberg, Jon and Margetts, Helen and Mullainathan, Sendhil and Salganik, Matthew J and Vazire, Simine and others},
  journal={Nature},
  volume={595},
  number={7866},
  pages={181--188},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{jackson1986mary,
  title={What Mary didn't know},
  author={Jackson, Frank},
  journal={The Journal of Philosophy},
  volume={83},
  number={5},
  pages={291--295},
  year={1986},
  publisher={JSTOR}
}

@article{jackson1982epiphenomenal,
  title={Epiphenomenal qualia},
  author={Jackson, Frank},
  journal={The Philosophical Quarterly (1950-)},
  volume={32},
  number={127},
  pages={127--136},
  year={1982},
  publisher={JSTOR}
}

@article{lazer2020computational,
  title={Computational social science: Obstacles and opportunities},
  author={Lazer, David MJ and Pentland, Alex and Watts, Duncan J and Aral, Sinan and Athey, Susan and Contractor, Noshir and Freelon, Deen and Gonzalez-Bailon, Sandra and King, Gary and Margetts, Helen and others},
  journal={Science},
  volume={369},
  number={6507},
  pages={1060--1062},
  year={2020},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{GoodfellowSS14,
  author    = {Ian J. Goodfellow and
               Jonathon Shlens and
               Christian Szegedy},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Explaining and Harnessing Adversarial Examples},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6572},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoodfellowSS14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{morris-etal-2020-reevaluating,
    title = "Reevaluating Adversarial Examples in Natural Language",
    author = "Morris, John  and
      Lifland, Eli  and
      Lanchantin, Jack  and
      Ji, Yangfeng  and
      Qi, Yanjun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.341",
    doi = "10.18653/v1/2020.findings-emnlp.341",
    pages = "3829--3839",
    abstract = "State-of-the-art attacks on NLP models lack a shared definition of a what constitutes a successful attack. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the model and follows some linguistic constraints. We then analyze the outputs of two state-of-the-art synonym substitution attacks. We find that their perturbations often do not preserve semantics, and 38{\%} introduce grammatical errors. Human surveys reveal that to successfully preserve semantics, we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences.With constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points.",
}


@article{van_der_vegt_multi-year_2022,
	title = {A multi-year study on insights into emotional coping during the pandemic},
	author = {van der Vegt, Isabelle and Mozes, Maximilian and Kleinberg, Bennett},
	year = {2022},
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{cer_universal_2018,
	address = {Brussels, Belgium},
	title = {Universal {Sentence} {Encoder} for {English}},
	url = {http://aclweb.org/anthology/D18-2029},
	doi = {10.18653/v1/D18-2029},
	abstract = {We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.},
	language = {en},
	urldate = {2021-04-11},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and St. John, Rhomni and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Strope, Brian and Kurzweil, Ray},
	year = {2018},
	pages = {169--174},
	file = {Cer et al. - 2018 - Universal Sentence Encoder for English.pdf:/Users/bennettkleinberg/Zotero/storage/BKY8E9U6/Cer et al. - 2018 - Universal Sentence Encoder for English.pdf:application/pdf},
}


@article{miotto_who_2022,
  title={Who is GPT-3? An Exploration of Personality, Values and Demographics},
  author={Miotto, Maril{\`u} and Rossberg, Nicola and Kleinberg, Bennett},
  journal={arXiv preprint arXiv:2209.14338},
  year={2022}
}

@misc{stevenson_putting_2022,
	title = {Putting {GPT}-3's {Creativity} to the ({Alternative} {Uses}) {Test}},
	url = {http://arxiv.org/abs/2206.08932},
	doi = {10.48550/arXiv.2206.08932},
	abstract = {AI large language models have (co-)produced amazing written works from newspaper articles to novels and poetry. These works meet the standards of the standard definition of creativity: being original and useful, and sometimes even the additional element of surprise. But can a large language model designed to predict the next text fragment provide creative, out-of-the-box, responses that still solve the problem at hand? We put Open AI's generative natural language model, GPT-3, to the test. Can it provide creative solutions to one of the most commonly used tests in creativity research? We assessed GPT-3's creativity on Guilford's Alternative Uses Test and compared its performance to previously collected human responses on expert ratings of originality, usefulness and surprise of responses, flexibility of each set of ideas as well as an automated method to measure creativity based on the semantic distance between a response and the AUT object in question. Our results show that -- on the whole -- humans currently outperform GPT-3 when it comes to creative output. But, we believe it is only a matter of time before GPT-3 catches up on this particular task. We discuss what this work reveals about human and AI creativity, creativity testing and our definition of creativity.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Stevenson, Claire and Smal, Iris and Baas, Matthijs and Grasman, Raoul and van der Maas, Han},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08932 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/PI9QHXNM/2206.html:text/html},
}

@article{binz_using_2022,
	title = {Using cognitive psychology to understand {GPT}-3},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2206.14576},
	doi = {10.48550/ARXIV.2206.14576},
	abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3's behavior is impressive: it solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multi-armed bandit task, and shows signatures of model-based reinforcement learning. Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. These results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
	urldate = {2022-09-18},
	author = {Binz, Marcel and Schulz, Eric},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{weber_does_2021,
	title = {Does {GPT}-3 have a theory of mind?},
	url = {https://medium.com/@julian78w/does-gpt-3-have-a-theory-of-mind-890fb0c7bf48},
	abstract = {Introduction},
	language = {en},
	urldate = {2022-09-18},
	journal = {Medium},
	author = {Weber, Julian},
	month = oct,
	year = {2021},
	file = {Snapshot:/Users/bennettkleinberg/Zotero/storage/RUCUNPYR/does-gpt-3-have-a-theory-of-mind-890fb0c7bf48.html:text/html},
}

@article{van_der_maas_how_2021,
	title = {How much intelligence is there in artificial intelligence? {A} 2020 update},
	volume = {87},
	issn = {01602896},
	shorttitle = {How much intelligence is there in artificial intelligence?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0160289621000325},
	doi = {10.1016/j.intell.2021.101548},
	language = {en},
	urldate = {2022-09-18},
	journal = {Intelligence},
	author = {van der Maas, Han L.J. and Snoek, Lukas and Stevenson, Claire E.},
	month = jul,
	year = {2021},
	pages = {101548},
}


@article{kleinberg_netanos-named_2017,
	title = {{NETANOS}-{Named} entity-based {Text} {Anonymization} for {Open} {Science}},
	url = {https://osf.io/preprints/w9nhb/},
	urldate = {2017-07-22},
	journal = {OSF Preprint},
	author = {Kleinberg, Bennett and Mozes, Maximilian and van der Toolen, Yaloe and Verschuere, Bruno},
	year = {2017},
	file = {[PDF] osf.io:/Users/bennettkleinberg/Zotero/storage/XVJ39PCI/Kleinberg et al. - 2017 - NETANOS-Named entity-based Text Anonymization for .pdf:application/pdf;Snapshot:/Users/bennettkleinberg/Zotero/storage/II35WF2W/w9nhb.html:text/html},
}

@article{kleinberg_textwash_2022,
	title = {Textwash - automated open-source text anonymisation},
	url = {http://arxiv.org/abs/2208.13081},
	doi = {10.48550/arXiv.2208.13081},
	abstract = {The increased use of text data in social science research has benefited from easy-to-access data (e.g., Twitter). That trend comes at the cost of research requiring sensitive but hard-to-share data (e.g., interview data, police reports, electronic health records). We introduce a solution to that stalemate with the open-source text anonymisation software\_Textwash\_. This paper presents the empirical evaluation of the tool using the TILD criteria: a technical evaluation (how accurate is the tool?), an information loss evaluation (how much information is lost in the anonymisation process?) and a de-anonymisation test (can humans identify individuals from anonymised text data?). The findings suggest that Textwash performs similar to state-of-the-art entity recognition models and introduces a negligible information loss of 0.84\%. For the de-anonymisation test, we tasked humans to identify individuals by name from a dataset of crowdsourced person descriptions of very famous, semi-famous and non-existing individuals. The de-anonymisation rate ranged from 1.01-2.01\% for the realistic use cases of the tool. We replicated the findings in a second study and concluded that Textwash succeeds in removing potentially sensitive information that renders detailed person descriptions practically anonymous.},
	urldate = {2022-09-13},
	journal = {arXiv:2208.13081},
	author = {Kleinberg, Bennett and Davies, Toby and Mozes, Maximilian},
	month = aug,
	year = {2022},
	note = {arXiv:2208.13081 [cs]
version: 1},
	keywords = {Computer Science - Computers and Society, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/LLU26FUN/2208.html:text/html},
}


@article{mozes_no_2021,
	title = {No {Intruder}, no {Validity}: {Evaluation} {Criteria} for {Privacy}-{Preserving} {Text} {Anonymization}},
	shorttitle = {No {Intruder}, no {Validity}},
	url = {http://arxiv.org/abs/2103.09263},
	abstract = {For sensitive text data to be shared among NLP researchers and practitioners, shared documents need to comply with data protection and privacy laws. There is hence a growing interest in automated approaches for text anonymization. However, measuring such methods' performance is challenging: missing a single identifying attribute can reveal an individual's identity. In this paper, we draw attention to this problem and argue that researchers and practitioners developing automated text anonymization systems should carefully assess whether their evaluation methods truly reflect the system's ability to protect individuals from being re-identified. We then propose TILD, a set of evaluation criteria that comprises an anonymization method's technical performance, the information loss resulting from its anonymization, and the human ability to de-anonymize redacted documents. These criteria may facilitate progress towards a standardized way for measuring anonymization performance.},
	urldate = {2021-03-18},
	journal = {arXiv:2103.09263 [cs]},
	author = {Mozes, Maximilian and Kleinberg, Bennett},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.09263},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/BX6RQ5W6/2103.html:text/html},
}

@article{rahwan2019machine,
  title={Machine behaviour},
  author={Rahwan, Iyad and Cebrian, Manuel and Obradovich, Nick and Bongard, Josh and Bonnefon, Jean-Fran{\c{c}}ois and Breazeal, Cynthia and Crandall, Jacob W and Christakis, Nicholas A and Couzin, Iain D and Jackson, Matthew O and others},
  journal={Nature},
  volume={568},
  number={7753},
  pages={477--486},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{shihadehbrilliance,
  title={Brilliance Bias in GPT-3},
  author={Shihadeh, Juliana and Ackerman, Margareta and Troske, Ashley and Lawson, Nicole and Gonzalez, Edith},
  year={2022}
}

@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}

@article{ashton2009hexaco,
  title={The HEXACO--60: A short measure of the major dimensions of personality},
  author={Ashton, Michael C and Lee, Kibeom},
  journal={Journal of personality assessment},
  volume={91},
  number={4},
  pages={340--345},
  year={2009},
  publisher={Taylor \& Francis}
}

@article{hvs,
  doi = {10.6102/zis234},
  url = {https://zis.gesis.org/DoiId/zis234},
  author = {Schwartz, S. H., Breyer, B., & Danner, D.},
  language = {en},
  title = {Human Values Scale (ESS)},
  journal = {Zusammenstellung sozialwissenschaftlicher Items und Skalen (ZIS)},
  publisher = {ZIS - GESIS Leibniz Institute for the Social Sciences},
  year = {2015}
}

@inproceedings{GoodfellowSS14,
  author    = {Ian J. Goodfellow and
               Jonathon Shlens and
               Christian Szegedy},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Explaining and Harnessing Adversarial Examples},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6572},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoodfellowSS14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{morris-etal-2020-reevaluating,
    title = "Reevaluating Adversarial Examples in Natural Language",
    author = "Morris, John  and
      Lifland, Eli  and
      Lanchantin, Jack  and
      Ji, Yangfeng  and
      Qi, Yanjun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.341",
    doi = "10.18653/v1/2020.findings-emnlp.341",
    pages = "3829--3839",
    abstract = "State-of-the-art attacks on NLP models lack a shared definition of a what constitutes a successful attack. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the model and follows some linguistic constraints. We then analyze the outputs of two state-of-the-art synonym substitution attacks. We find that their perturbations often do not preserve semantics, and 38{\%} introduce grammatical errors. Human surveys reveal that to successfully preserve semantics, we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences.With constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points.",
}

@inproceedings{mozes-etal-2021-contrasting,
    title = "Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification",
    author = "Mozes, Maximilian  and
      Bartolo, Max  and
      Stenetorp, Pontus  and
      Kleinberg, Bennett  and
      Griffin, Lewis",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.651",
    doi = "10.18653/v1/2021.emnlp-main.651",
    pages = "8258--8270",
    abstract = "Research shows that natural language processing models are generally considered to be vulnerable to adversarial attacks; but recent work has drawn attention to the issue of validating these adversarial inputs against certain criteria (e.g., the preservation of semantics and grammaticality). Enforcing constraints to uphold such criteria may render attacks unsuccessful, raising the question of whether valid attacks are actually feasible. In this work, we investigate this through the lens of human language ability. We report on crowdsourcing studies in which we task humans with iteratively modifying words in an input text, while receiving immediate model feedback, with the aim of causing a sentiment classification model to misclassify the example. Our findings suggest that humans are capable of generating a substantial amount of adversarial examples using semantics-preserving word substitutions. We analyze how human-generated adversarial examples compare to the recently proposed TextFooler, Genetic, BAE and SememePSO attack algorithms on the dimensions naturalness, preservation of sentiment, grammaticality and substitution rate. Our findings suggest that human-generated adversarial examples are not more able than the best algorithms to generate natural-reading, sentiment-preserving examples, though they do so by being much more computationally efficient.",
}

@inproceedings{mozes-etal-2021-frequency,
    title = "Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples",
    author = "Mozes, Maximilian  and
      Stenetorp, Pontus  and
      Kleinberg, Bennett  and
      Griffin, Lewis",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.13",
    doi = "10.18653/v1/2021.eacl-main.13",
    pages = "171--186",
    abstract = "Recent efforts have shown that neural text processing models are vulnerable to adversarial examples, but the nature of these examples is poorly understood. In this work, we show that adversarial attacks against CNN, LSTM and Transformer-based classification models perform word substitutions that are identifiable through frequency differences between replaced words and their corresponding substitutions. Based on these findings, we propose frequency-guided word substitutions (FGWS), a simple algorithm exploiting the frequency properties of adversarial word substitutions for the detection of adversarial examples. FGWS achieves strong performance by accurately detecting adversarial examples on the SST-2 and IMDb sentiment datasets, with F1 detection scores of up to 91.4{\%} against RoBERTa-based classification models. We compare our approach against a recently proposed perturbation discrimination framework and show that we outperform it by up to 13.0{\%} F1.",
}


@article{mozes_identifying_2022,
	title = {Identifying {Human} {Strategies} for {Generating} {Word}-{Level} {Adversarial} {Examples}},
	journal = {Findings of EMNLP 2022},
	author = {Mozes, Maximilian and Kleinberg, Bennett and Griffin, Lewis},
	year = {2022},
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?ðŸ¦œ},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@article{garg2018word,
  title={Word embeddings quantify 100 years of gender and ethnic stereotypes},
  author={Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={16},
  pages={E3635--E3644},
  year={2018},
  publisher={National Acad Sciences}
}

@article{argyle2022out,
  title={Out of One, Many: Using Language Models to Simulate Human Samples},
  author={Argyle, Lisa P and Busby, Ethan C and Fulda, Nancy and Gubler, Joshua and Rytting, Christopher and Wingate, David},
  journal={arXiv preprint arXiv:2209.06899},
  year={2022}
}

@article{jin2022make,
  title={When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment},
  author={Jin, Zhijing and Levine, Sydney and Gonzalez, Fernando and Kamal, Ojasv and Sap, Maarten and Sachan, Mrinmaya and Mihalcea, Rada and Tenenbaum, Josh and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:2210.01478},
  year={2022}
}

@article{sen2019total,
  title={A total error framework for digital traces of humans},
  author={Sen, Indira and Floeck, Fabian and Weller, Katrin and Weiss, Bernd and Wagner, Claudia},
  journal={arXiv preprint arXiv:1907.08228},
  year={2019}
}

@inproceedings{mitchell2019model,
  title={Model cards for model reporting},
  author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={220--229},
  year={2019}
}

@article{oswald2018algorithmic,
  title={Algorithmic risk assessment policing models: lessons from the Durham HART model and â€˜Experimentalâ€™proportionality},
  author={Oswald, Marion and Grace, Jamie and Urwin, Sheena and Barnes, Geoffrey C},
  journal={Information \& Communications Technology Law},
  volume={27},
  number={2},
  pages={223--250},
  year={2018},
  publisher={Taylor \& Francis}
}

@article{tversky1983extensional,
  title={Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment.},
  author={Tversky, Amos and Kahneman, Daniel},
  journal={Psychological review},
  volume={90},
  number={4},
  pages={293},
  year={1983},
  publisher={American Psychological Association}
}

@article{frederick2005cognitive,
  title={Cognitive reflection and decision making},
  author={Frederick, Shane},
  journal={Journal of Economic perspectives},
  volume={19},
  number={4},
  pages={25--42},
  year={2005}
}

@article{huang2022folk,
  title={Folk Theories of Online Dating: Exploring Peopleâ€™s Beliefs About the Online Dating Process and Online Dating Algorithms},
  author={Huang, Sabrina Angela and Hancock, Jeffrey and Tong, Stephanie Tom},
  journal={Social Media+ Society},
  volume={8},
  number={2},
  pages={20563051221089561},
  year={2022},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{devito2018people,
  title={How people form folk theories of social media feeds and what it means for how we study self-presentation},
  author={DeVito, Michael A and Birnholtz, Jeremy and Hancock, Jeffery T and French, Megan and Liu, Sunny},
  booktitle={Proceedings of the 2018 CHI conference on human factors in computing systems},
  pages={1--12},
  year={2018}
}

@inproceedings{devito2018algorithm,
  title={The algorithm and the user: How can hci use lay understandings of algorithmic systems?},
  author={DeVito, Michael A and Hancock, Jeffrey T and French, Megan and Birnholtz, Jeremy and Antin, Judd and Karahalios, Karrie and Tong, Stephanie and Shklovski, Irina},
  booktitle={Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
  pages={1--6},
  year={2018}
}

@article{jin2022make,
  title={When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment},
  author={Jin, Zhijing and Levine, Sydney and Gonzalez, Fernando and Kamal, Ojasv and Sap, Maarten and Sachan, Mrinmaya and Mihalcea, Rada and Tenenbaum, Josh and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:2210.01478},
  year={2022}
}

@article{sap2022neural,
  title={Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs},
  author={Sap, Maarten and LeBras, Ronan and Fried, Daniel and Choi, Yejin},
  journal={arXiv preprint arXiv:2210.13312},
  year={2022}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{goldstein2022shared,
  title={Shared computational principles for language processing in humans and deep language models},
  author={Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A and Feder, Amir and Emanuel, Dotan and Cohen, Alon and others},
  journal={Nature neuroscience},
  volume={25},
  number={3},
  pages={369--380},
  year={2022},
  publisher={Nature Publishing Group}
}

@inproceedings{bender2020climbing,
  title={Climbing towards NLU: On meaning, form, and understanding in the age of data},
  author={Bender, Emily M and Koller, Alexander},
  booktitle={Proceedings of the 58th annual meeting of the association for computational linguistics},
  pages={5185--5198},
  year={2020}
}

@article{henderson2022reproducible,
  title={A reproducible systematic map of research on the illusory truth effect},
  author={Henderson, Emma L and Westwood, Samuel J and Simons, Daniel J},
  journal={Psychonomic Bulletin \& Review},
  volume={29},
  number={3},
  pages={1065--1088},
  year={2022},
  publisher={Springer}
}

@inproceedings{lucy2021gender,
  title={Gender and representation bias in GPT-3 generated stories},
  author={Lucy, Li and Bamman, David},
  booktitle={Proceedings of the Third Workshop on Narrative Understanding},
  pages={48--55},
  year={2021}
}

@inproceedings{abid2021persistent,
  title={Persistent anti-muslim bias in large language models},
  author={Abid, Abubakar and Farooqi, Maheen and Zou, James},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={298--306},
  year={2021}
}

@article{rottweiler2021measuring,
  title={Measuring Individualsâ€™ Misogynistic Attitudes: Development and Validation of the Misogyny Scale},
  author={Rottweiler, Bettina and Gill, Paul},
  year={2021},
  publisher={PsyArXiv}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{van_der_vegt_multi-year_2022,
	title = {A multi-year study on insights into emotional coping during the pandemic},
	author = {van der Vegt, Isabelle and Mozes, Maximilian and Kleinberg, Bennett},
	year = {2022},
}


@article{okamura_adaptive_2020,
	title = {Adaptive trust calibration for human-{AI} collaboration},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0229132},
	doi = {10.1371/journal.pone.0229132},
	language = {en},
	number = {2},
	urldate = {2024-08-26},
	journal = {PLOS ONE},
	author = {Okamura, Kazuo and Yamada, Seiji},
	editor = {Lv, Chen},
	month = feb,
	year = {2020},
	pages = {e0229132},
}


@article{lee_trust_2004,
	title = {Trust in {Automation}: {Designing} for {Appropriate} {Reliance}},
	volume = {46},
	issn = {0018-7208},
	shorttitle = {Trust in {Automation}},
	url = {http://hfs.sagepub.com/cgi/doi/10.1518/hfes.46.1.50_30392},
	doi = {10.1518/hfes.46.1.50_30392},
	language = {en},
	number = {1},
	urldate = {2024-08-26},
	journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	author = {Lee, J. D. and See, K. A.},
	month = jan,
	year = {2004},
	pages = {50--80},
}




@misc{hagendorff_machine_2023,
	title = {Machine {Psychology}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.13988},
	doi = {10.48550/ARXIV.2303.13988},
	abstract = {Large language models (LLMs) show increasingly advanced emergent capabilities and are being incorporated across various societal domains. Understanding their behavior and reasoning abilities therefore holds significant importance. We argue that a fruitful direction for research is engaging LLMs in behavioral experiments inspired by psychology that have traditionally been aimed at understanding human cognition and behavior. In this article, we highlight and summarize theoretical perspectives, experimental paradigms, and computational analysis techniques that this approach brings to the table. It paves the way for a "machine psychology" for generative artificial intelligence (AI) that goes beyond performance benchmarks and focuses instead on computational insights that move us toward a better understanding and discovery of emergent abilities and behavioral patterns in LLMs. We review existing work taking this approach, synthesize best practices, and highlight promising future directions. We also highlight the important caveats of applying methodologies designed for understanding humans to machines. We posit that leveraging tools from experimental psychology to study AI will become increasingly valuable as models evolve to be more powerful, opaque, multi-modal, and integrated into complex real-world settings.},
	urldate = {2024-08-25},
	publisher = {arXiv},
	author = {Hagendorff, Thilo and Dasgupta, Ishita and Binz, Marcel and Chan, Stephanie C. Y. and Lampinen, Andrew and Wang, Jane X. and Akata, Zeynep and Schulz, Eric},
	year = {2023},
	note = {Version Number: 6},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@article{tolman_cognitive_1948,
	title = {Cognitive maps in rats and men.},
	volume = {55},
	issn = {1939-1471, 0033-295X},
	url = {https://doi.apa.org/doi/10.1037/h0061626},
	doi = {10.1037/h0061626},
	language = {en},
	number = {4},
	urldate = {2024-09-09},
	journal = {Psychological Review},
	author = {Tolman, Edward C.},
	year = {1948},
	pages = {189--208},
}

@book{chomsky_syntactic_2002,
	title = {Syntactic {Structures}},
	isbn = {978-3-11-017279-9},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110218329/html},
	urldate = {2024-09-09},
	publisher = {Mouton de Gruyter},
	author = {Chomsky, Noam},
	month = nov,
	year = {2002},
	doi = {10.1515/9783110218329},
}

@incollection{epstein_computing_2009,
	address = {Dordrecht},
	title = {Computing {Machinery} and {Intelligence}},
	isbn = {978-1-4020-9624-2 978-1-4020-6710-5},
	url = {http://link.springer.com/10.1007/978-1-4020-6710-5_3},
	language = {en},
	urldate = {2024-09-09},
	booktitle = {Parsing the {Turing} {Test}},
	publisher = {Springer Netherlands},
	author = {Turing, Alan M.},
	editor = {Epstein, Robert and Roberts, Gary and Beber, Grace},
	year = {2009},
	doi = {10.1007/978-1-4020-6710-5_3},
	pages = {23--65},
}

@article{miller_magical_1956,
	title = {The magical number seven, plus or minus two: {Some} limits on our capacity for processing information.},
	volume = {63},
	issn = {1939-1471, 0033-295X},
	shorttitle = {The magical number seven, plus or minus two},
	url = {https://doi.apa.org/doi/10.1037/h0043158},
	doi = {10.1037/h0043158},
	language = {en},
	number = {2},
	urldate = {2024-09-09},
	journal = {Psychological Review},
	author = {Miller, George A.},
	month = mar,
	year = {1956},
	pages = {81--97},
}

@article{miller_cognitive_2003,
	title = {The cognitive revolution: a historical perspective},
	volume = {7},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {13646613},
	shorttitle = {The cognitive revolution},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661303000299},
	doi = {10.1016/S1364-6613(03)00029-9},
	language = {en},
	number = {3},
	urldate = {2024-09-09},
	journal = {Trends in Cognitive Sciences},
	author = {Miller, George A},
	month = mar,
	year = {2003},
	pages = {141--144},
}

@article{tolman_introduction_1930,
	title = {Introduction and removal of reward, and maze performance in rats.},
	volume = {4},
	abstract = {Two groups of rats, one which ran the maze with reward, and one which ran the maze without reward, were tested to determine the influence upon the learning curve of a sudden removal, or a sudden introduction, of a food reward. The maze was a 14-unit T-maze. Reliability coefficients ranged from .876 to .965. When the food reward was removed from the maze the error scores and time scores of the rewarded rats showed a large increase. When reward was introduced into the maze the non-rewarded rats showed a large decrease in time and error scores. "The drop in the error curve for the group of rats that were rewarded on the eleventh day brought the curve significantly below the curve of a control group of rats that had been rewarded from the first. This suggests that latent learning may be more effective than overt learning." The relative difficulty of the various blinds shifted as a result of the introduction or removal of rÃ©ward. The reliability of the maze was higher under reward than under non-reward conditions. Even under non-reward conditions, however, the reliability coefficients based on even-day versus odd-day scores in errors and time were all above .876 Â± .024. Bibliography. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {University of California Publications in Psychology},
	author = {Tolman, E. C. and Honzik, C. H.},
	year = {1930},
	pages = {257--275},
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@misc{mozes_use_2023,
	title = {Use of {LLMs} for {Illicit} {Purposes}: {Threats}, {Prevention} {Measures}, and {Vulnerabilities}},
	shorttitle = {Use of {LLMs} for {Illicit} {Purposes}},
	url = {http://arxiv.org/abs/2308.12833},
	abstract = {Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures. With our work, we hope to raise awareness of the limitations of LLMs in light of such security concerns, among both experienced developers and novel users of such technologies.},
	urldate = {2023-08-28},
	publisher = {arXiv},
	author = {Mozes, Maximilian and He, Xuanli and Kleinberg, Bennett and Griffin, Lewis D.},
	month = aug,
	year = {2023},
	note = {arXiv:2308.12833 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/4WRWP8WA/2308.html:text/html},
}



@article{gentzkow_text_2019,
	title = {Text as {Data}},
	volume = {57},
	issn = {0022-0515},
	url = {https://pubs.aeaweb.org/doi/10.1257/jel.20181020},
	doi = {10.1257/jel.20181020},
	language = {en},
	number = {3},
	urldate = {2020-04-04},
	journal = {Journal of Economic Literature},
	author = {Gentzkow, Matthew and Kelly, Bryan and Taddy, Matt},
	month = sep,
	year = {2019},
	pages = {535--574},
}

@incollection{aref_women_2020,
	address = {Cham},
	title = {Women {Worry} {About} {Family}, {Men} {About} the {Economy}: {Gender} {Differences} in {Emotional} {Responses} to {COVID}-19},
	volume = {12467},
	isbn = {978-3-030-60974-0 978-3-030-60975-7},
	shorttitle = {Women {Worry} {About} {Family}, {Men} {About} the {Economy}},
	url = {http://link.springer.com/10.1007/978-3-030-60975-7_29},
	language = {en},
	urldate = {2020-10-25},
	booktitle = {Social {Informatics}},
	publisher = {Springer},
	author = {van der Vegt, Isabelle and Kleinberg, Bennett},
	editor = {Aref, Samin and Bontcheva, Kalina and Braghieri, Marco and Dignum, Frank and Giannotti, Fosca and Grisolia, Francesco and Pedreschi, Dino},
	year = {2020},
	doi = {10.1007/978-3-030-60975-7_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {397--409},
}

@inproceedings{kleinberg_measuring_2020,
	address = {Online},
	title = {Measuring {Emotions} in the {COVID}-19 {Real} {World} {Worry} {Dataset}},
	url = {https://www.aclweb.org/anthology/2020.nlpcovid19-acl.11},
	abstract = {The COVID-19 pandemic is having a dramatic impact on societies and economies around the world. With various measures of lockdowns and social distancing in place, it becomes important to understand emotional responses on a large scale. In this paper, we present the first ground truth dataset of emotional responses to COVID-19. We asked participants to indicate their emotions and express these in text. This resulted in the Real World Worry Dataset of 5,000 texts (2,500 short + 2,500 long texts). Our analyses suggest that emotional responses correlated with linguistic measures. Topic modeling further revealed that people in the UK worry about their family and the economic situation. Tweet-sized texts functioned as a call for solidarity, while longer texts shed light on worries and concerns. Using predictive modeling approaches, we were able to approximate the emotional responses of participants from text within 14\% of their actual value. We encourage others to use the dataset and improve how we can use automated methods to learn about emotional responses and worries about an urgent problem.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 1st {Workshop} on {NLP} for {COVID}-19 at {ACL} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Kleinberg, Bennett and van der Vegt, Isabelle and Mozes, Maximilian},
	month = jul,
	year = {2020},
}

@book{salganik_bit_2019,
	address = {Princeton, NJ},
	title = {Bit by bit: {Social} research in the digital age},
	isbn = {978-0-691-19610-7},
	shorttitle = {Bit by bit},
	language = {English},
	publisher = {Princeton University Press},
	author = {Salganik, Matthew J},
	year = {2019},
	note = {OCLC: 1134658838},
}

@article{boyd_natural_2021,
	title = {Natural {Language} {Analysis} and the {Psychology} of {Verbal} {Behavior}: {The} {Past}, {Present}, and {Future} {States} of the {Field}},
	volume = {40},
	issn = {0261-927X},
	shorttitle = {Natural {Language} {Analysis} and the {Psychology} of {Verbal} {Behavior}},
	url = {https://doi.org/10.1177/0261927X20967028},
	doi = {10.1177/0261927X20967028},
	abstract = {Throughout history, scholars and laypeople alike have believed that our words contain subtle clues about what we are like as people, psychologically speaking. However, the ways in which language has been used to infer psychological processes has seen dramatic shifts over time and, with modern computational technologies and digital data sources, we are on the verge of a massive revolution in language analysis research. In this article, we discuss the past and current states of research at the intersection of language analysis and psychology, summarizing the central successes and shortcomings of psychological text analysis to date. We additionally outline and discuss a critical need for language analysis practitioners in the social sciences to expand their view of verbal behavior. Lastly, we discuss the trajectory of interdisciplinary research on language and the challenges of integrating analysis methods across paradigms, recommending promising future directions for the field along the way.},
	language = {en},
	number = {1},
	urldate = {2021-08-18},
	journal = {Journal of Language and Social Psychology},
	author = {Boyd, Ryan L. and Schwartz, H. Andrew},
	month = jan,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	keywords = {attention, computational social science, language analysis, natural language processing},
	pages = {21--41},
}

@article{mozes_repeated-measures_2021,
	title = {A repeated-measures study on emotional responses after a year in the pandemic},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-02414-9},
	doi = {10.1038/s41598-021-02414-9},
	abstract = {The introduction of COVID-19 lockdown measures and an outlook on return to normality are demanding societal changes. Among the most pressing questions is how individuals adjust to the pandemic. This paper examines the emotional responses to the pandemic in a repeated-measures design. Data (nâ€‰=â€‰1698) were collected in April 2020 (during strict lockdown measures) and in April 2021 (when vaccination programmes gained traction). We asked participants to report their emotions and express these in text data. Statistical tests revealed an average trend towards better adjustment to the pandemic. However, clustering analyses suggested a more complex heterogeneous pattern with a well-coping and a resigning subgroup of participants. Linguistic computational analyses uncovered that topics and n-gram frequencies shifted towards attention to the vaccination programme and away from general worrying. Implications for public mental health efforts in identifying people at heightened risk are discussed. The dataset is made publicly available.},
	language = {en},
	number = {1},
	urldate = {2021-12-03},
	journal = {Scientific Reports},
	author = {Mozes, Maximilian and van der Vegt, Isabelle and Kleinberg, Bennett},
	month = nov,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Human behaviour;Population screening
Subject\_term\_id: human-behaviour;population-screening},
	keywords = {Human behaviour, Population screening},
	pages = {23114},
	file = {Snapshot:/Users/bennettkleinberg/Zotero/storage/6B6EY4IH/s41598-021-02414-9.html:text/html},
}

@inproceedings{morris-etal-2020-textattack,
    title = "{T}ext{A}ttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in {NLP}",
    author = "Morris, John  and
      Lifland, Eli  and
      Yoo, Jin Yong  and
      Grigsby, Jake  and
      Jin, Di  and
      Qi, Yanjun",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.16",
    doi = "10.18653/v1/2020.emnlp-demos.16",
    pages = "119--126",
    abstract = "While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack{'}s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.",
}

@inproceedings{mozes-etal-2021-contrasting,
    title = "Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification",
    author = "Mozes, Maximilian  and
      Bartolo, Max  and
      Stenetorp, Pontus  and
      Kleinberg, Bennett  and
      Griffin, Lewis",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.651",
    doi = "10.18653/v1/2021.emnlp-main.651",
    pages = "8258--8270",
    abstract = "Research shows that natural language processing models are generally considered to be vulnerable to adversarial attacks; but recent work has drawn attention to the issue of validating these adversarial inputs against certain criteria (e.g., the preservation of semantics and grammaticality). Enforcing constraints to uphold such criteria may render attacks unsuccessful, raising the question of whether valid attacks are actually feasible. In this work, we investigate this through the lens of human language ability. We report on crowdsourcing studies in which we task humans with iteratively modifying words in an input text, while receiving immediate model feedback, with the aim of causing a sentiment classification model to misclassify the example. Our findings suggest that humans are capable of generating a substantial amount of adversarial examples using semantics-preserving word substitutions. We analyze how human-generated adversarial examples compare to the recently proposed TextFooler, Genetic, BAE and SememePSO attack algorithms on the dimensions naturalness, preservation of sentiment, grammaticality and substitution rate. Our findings suggest that human-generated adversarial examples are not more able than the best algorithms to generate natural-reading, sentiment-preserving examples, though they do so by being much more computationally efficient.",
}

@inproceedings{mozes-etal-2021-frequency,
    title = "Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples",
    author = "Mozes, Maximilian  and
      Stenetorp, Pontus  and
      Kleinberg, Bennett  and
      Griffin, Lewis",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.13",
    doi = "10.18653/v1/2021.eacl-main.13",
    pages = "171--186",
    abstract = "Recent efforts have shown that neural text processing models are vulnerable to adversarial examples, but the nature of these examples is poorly understood. In this work, we show that adversarial attacks against CNN, LSTM and Transformer-based classification models perform word substitutions that are identifiable through frequency differences between replaced words and their corresponding substitutions. Based on these findings, we propose frequency-guided word substitutions (FGWS), a simple algorithm exploiting the frequency properties of adversarial word substitutions for the detection of adversarial examples. FGWS achieves strong performance by accurately detecting adversarial examples on the SST-2 and IMDb sentiment datasets, with F1 detection scores of up to 91.4{\%} against RoBERTa-based classification models. We compare our approach against a recently proposed perturbation discrimination framework and show that we outperform it by up to 13.0{\%} F1.",
}

@inproceedings{alzantot-etal-2018-generating,
    title = "Generating Natural Language Adversarial Examples",
    author = "Alzantot, Moustafa  and
      Sharma, Yash  and
      Elgohary, Ahmed  and
      Ho, Bo-Jhang  and
      Srivastava, Mani  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1316",
    doi = "10.18653/v1/D18-1316",
    pages = "2890--2896",
    abstract = "Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97{\%} and 70{\%}, respectively. We additionally demonstrate that 92.3{\%} of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.",
}

@article{hofman2021integrating,
  title={Integrating explanation and prediction in computational social science},
  author={Hofman, Jake M and Watts, Duncan J and Athey, Susan and Garip, Filiz and Griffiths, Thomas L and Kleinberg, Jon and Margetts, Helen and Mullainathan, Sendhil and Salganik, Matthew J and Vazire, Simine and others},
  journal={Nature},
  volume={595},
  number={7866},
  pages={181--188},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{jackson1986mary,
  title={What Mary didn't know},
  author={Jackson, Frank},
  journal={The Journal of Philosophy},
  volume={83},
  number={5},
  pages={291--295},
  year={1986},
  publisher={JSTOR}
}

@article{jackson1982epiphenomenal,
  title={Epiphenomenal qualia},
  author={Jackson, Frank},
  journal={The Philosophical Quarterly (1950-)},
  volume={32},
  number={127},
  pages={127--136},
  year={1982},
  publisher={JSTOR}
}

@article{lazer2020computational,
  title={Computational social science: Obstacles and opportunities},
  author={Lazer, David MJ and Pentland, Alex and Watts, Duncan J and Aral, Sinan and Athey, Susan and Contractor, Noshir and Freelon, Deen and Gonzalez-Bailon, Sandra and King, Gary and Margetts, Helen and others},
  journal={Science},
  volume={369},
  number={6507},
  pages={1060--1062},
  year={2020},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{GoodfellowSS14,
  author    = {Ian J. Goodfellow and
               Jonathon Shlens and
               Christian Szegedy},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Explaining and Harnessing Adversarial Examples},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6572},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoodfellowSS14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{morris-etal-2020-reevaluating,
    title = "Reevaluating Adversarial Examples in Natural Language",
    author = "Morris, John  and
      Lifland, Eli  and
      Lanchantin, Jack  and
      Ji, Yangfeng  and
      Qi, Yanjun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.341",
    doi = "10.18653/v1/2020.findings-emnlp.341",
    pages = "3829--3839",
    abstract = "State-of-the-art attacks on NLP models lack a shared definition of a what constitutes a successful attack. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the model and follows some linguistic constraints. We then analyze the outputs of two state-of-the-art synonym substitution attacks. We find that their perturbations often do not preserve semantics, and 38{\%} introduce grammatical errors. Human surveys reveal that to successfully preserve semantics, we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences.With constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points.",
}


@article{van_der_vegt_multi-year_2022,
	title = {A multi-year study on insights into emotional coping during the pandemic},
	author = {van der Vegt, Isabelle and Mozes, Maximilian and Kleinberg, Bennett},
	year = {2022},
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{cer_universal_2018,
	address = {Brussels, Belgium},
	title = {Universal {Sentence} {Encoder} for {English}},
	url = {http://aclweb.org/anthology/D18-2029},
	doi = {10.18653/v1/D18-2029},
	abstract = {We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.},
	language = {en},
	urldate = {2021-04-11},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and St. John, Rhomni and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Strope, Brian and Kurzweil, Ray},
	year = {2018},
	pages = {169--174},
	file = {Cer et al. - 2018 - Universal Sentence Encoder for English.pdf:/Users/bennettkleinberg/Zotero/storage/BKY8E9U6/Cer et al. - 2018 - Universal Sentence Encoder for English.pdf:application/pdf},
}


@article{miotto_who_2022,
  title={Who is GPT-3? An Exploration of Personality, Values and Demographics},
  author={Miotto, Maril{\`u} and Rossberg, Nicola and Kleinberg, Bennett},
  journal={arXiv preprint arXiv:2209.14338},
  year={2022}
}

@misc{stevenson_putting_2022,
	title = {Putting {GPT}-3's {Creativity} to the ({Alternative} {Uses}) {Test}},
	url = {http://arxiv.org/abs/2206.08932},
	doi = {10.48550/arXiv.2206.08932},
	abstract = {AI large language models have (co-)produced amazing written works from newspaper articles to novels and poetry. These works meet the standards of the standard definition of creativity: being original and useful, and sometimes even the additional element of surprise. But can a large language model designed to predict the next text fragment provide creative, out-of-the-box, responses that still solve the problem at hand? We put Open AI's generative natural language model, GPT-3, to the test. Can it provide creative solutions to one of the most commonly used tests in creativity research? We assessed GPT-3's creativity on Guilford's Alternative Uses Test and compared its performance to previously collected human responses on expert ratings of originality, usefulness and surprise of responses, flexibility of each set of ideas as well as an automated method to measure creativity based on the semantic distance between a response and the AUT object in question. Our results show that -- on the whole -- humans currently outperform GPT-3 when it comes to creative output. But, we believe it is only a matter of time before GPT-3 catches up on this particular task. We discuss what this work reveals about human and AI creativity, creativity testing and our definition of creativity.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Stevenson, Claire and Smal, Iris and Baas, Matthijs and Grasman, Raoul and van der Maas, Han},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08932 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/PI9QHXNM/2206.html:text/html},
}

@article{binz_using_2022,
	title = {Using cognitive psychology to understand {GPT}-3},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2206.14576},
	doi = {10.48550/ARXIV.2206.14576},
	abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3's behavior is impressive: it solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multi-armed bandit task, and shows signatures of model-based reinforcement learning. Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. These results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
	urldate = {2022-09-18},
	author = {Binz, Marcel and Schulz, Eric},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{weber_does_2021,
	title = {Does {GPT}-3 have a theory of mind?},
	url = {https://medium.com/@julian78w/does-gpt-3-have-a-theory-of-mind-890fb0c7bf48},
	abstract = {Introduction},
	language = {en},
	urldate = {2022-09-18},
	journal = {Medium},
	author = {Weber, Julian},
	month = oct,
	year = {2021},
	file = {Snapshot:/Users/bennettkleinberg/Zotero/storage/RUCUNPYR/does-gpt-3-have-a-theory-of-mind-890fb0c7bf48.html:text/html},
}

@article{van_der_maas_how_2021,
	title = {How much intelligence is there in artificial intelligence? {A} 2020 update},
	volume = {87},
	issn = {01602896},
	shorttitle = {How much intelligence is there in artificial intelligence?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0160289621000325},
	doi = {10.1016/j.intell.2021.101548},
	language = {en},
	urldate = {2022-09-18},
	journal = {Intelligence},
	author = {van der Maas, Han L.J. and Snoek, Lukas and Stevenson, Claire E.},
	month = jul,
	year = {2021},
	pages = {101548},
}


@article{kleinberg_netanos-named_2017,
	title = {{NETANOS}-{Named} entity-based {Text} {Anonymization} for {Open} {Science}},
	url = {https://osf.io/preprints/w9nhb/},
	urldate = {2017-07-22},
	journal = {OSF Preprint},
	author = {Kleinberg, Bennett and Mozes, Maximilian and van der Toolen, Yaloe and Verschuere, Bruno},
	year = {2017},
	file = {[PDF] osf.io:/Users/bennettkleinberg/Zotero/storage/XVJ39PCI/Kleinberg et al. - 2017 - NETANOS-Named entity-based Text Anonymization for .pdf:application/pdf;Snapshot:/Users/bennettkleinberg/Zotero/storage/II35WF2W/w9nhb.html:text/html},
}

@article{kleinberg_textwash_2022,
	title = {Textwash - automated open-source text anonymisation},
	url = {http://arxiv.org/abs/2208.13081},
	doi = {10.48550/arXiv.2208.13081},
	abstract = {The increased use of text data in social science research has benefited from easy-to-access data (e.g., Twitter). That trend comes at the cost of research requiring sensitive but hard-to-share data (e.g., interview data, police reports, electronic health records). We introduce a solution to that stalemate with the open-source text anonymisation software\_Textwash\_. This paper presents the empirical evaluation of the tool using the TILD criteria: a technical evaluation (how accurate is the tool?), an information loss evaluation (how much information is lost in the anonymisation process?) and a de-anonymisation test (can humans identify individuals from anonymised text data?). The findings suggest that Textwash performs similar to state-of-the-art entity recognition models and introduces a negligible information loss of 0.84\%. For the de-anonymisation test, we tasked humans to identify individuals by name from a dataset of crowdsourced person descriptions of very famous, semi-famous and non-existing individuals. The de-anonymisation rate ranged from 1.01-2.01\% for the realistic use cases of the tool. We replicated the findings in a second study and concluded that Textwash succeeds in removing potentially sensitive information that renders detailed person descriptions practically anonymous.},
	urldate = {2022-09-13},
	journal = {arXiv:2208.13081},
	author = {Kleinberg, Bennett and Davies, Toby and Mozes, Maximilian},
	month = aug,
	year = {2022},
	note = {arXiv:2208.13081 [cs]
version: 1},
	keywords = {Computer Science - Computers and Society, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/LLU26FUN/2208.html:text/html},
}


@article{mozes_no_2021,
	title = {No {Intruder}, no {Validity}: {Evaluation} {Criteria} for {Privacy}-{Preserving} {Text} {Anonymization}},
	shorttitle = {No {Intruder}, no {Validity}},
	url = {http://arxiv.org/abs/2103.09263},
	abstract = {For sensitive text data to be shared among NLP researchers and practitioners, shared documents need to comply with data protection and privacy laws. There is hence a growing interest in automated approaches for text anonymization. However, measuring such methods' performance is challenging: missing a single identifying attribute can reveal an individual's identity. In this paper, we draw attention to this problem and argue that researchers and practitioners developing automated text anonymization systems should carefully assess whether their evaluation methods truly reflect the system's ability to protect individuals from being re-identified. We then propose TILD, a set of evaluation criteria that comprises an anonymization method's technical performance, the information loss resulting from its anonymization, and the human ability to de-anonymize redacted documents. These criteria may facilitate progress towards a standardized way for measuring anonymization performance.},
	urldate = {2021-03-18},
	journal = {arXiv:2103.09263 [cs]},
	author = {Mozes, Maximilian and Kleinberg, Bennett},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.09263},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/BX6RQ5W6/2103.html:text/html},
}

@article{rahwan2019machine,
  title={Machine behaviour},
  author={Rahwan, Iyad and Cebrian, Manuel and Obradovich, Nick and Bongard, Josh and Bonnefon, Jean-Fran{\c{c}}ois and Breazeal, Cynthia and Crandall, Jacob W and Christakis, Nicholas A and Couzin, Iain D and Jackson, Matthew O and others},
  journal={Nature},
  volume={568},
  number={7753},
  pages={477--486},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{shihadehbrilliance,
  title={Brilliance Bias in GPT-3},
  author={Shihadeh, Juliana and Ackerman, Margareta and Troske, Ashley and Lawson, Nicole and Gonzalez, Edith},
  year={2022}
}

@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}

@article{ashton2009hexaco,
  title={The HEXACO--60: A short measure of the major dimensions of personality},
  author={Ashton, Michael C and Lee, Kibeom},
  journal={Journal of personality assessment},
  volume={91},
  number={4},
  pages={340--345},
  year={2009},
  publisher={Taylor \& Francis}
}

@article{hvs,
  doi = {10.6102/zis234},
  url = {https://zis.gesis.org/DoiId/zis234},
  author = {Schwartz, S. H., Breyer, B., & Danner, D.},
  language = {en},
  title = {Human Values Scale (ESS)},
  journal = {Zusammenstellung sozialwissenschaftlicher Items und Skalen (ZIS)},
  publisher = {ZIS - GESIS Leibniz Institute for the Social Sciences},
  year = {2015}
}

@inproceedings{GoodfellowSS14,
  author    = {Ian J. Goodfellow and
               Jonathon Shlens and
               Christian Szegedy},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Explaining and Harnessing Adversarial Examples},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6572},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoodfellowSS14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{morris-etal-2020-reevaluating,
    title = "Reevaluating Adversarial Examples in Natural Language",
    author = "Morris, John  and
      Lifland, Eli  and
      Lanchantin, Jack  and
      Ji, Yangfeng  and
      Qi, Yanjun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.341",
    doi = "10.18653/v1/2020.findings-emnlp.341",
    pages = "3829--3839",
    abstract = "State-of-the-art attacks on NLP models lack a shared definition of a what constitutes a successful attack. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the model and follows some linguistic constraints. We then analyze the outputs of two state-of-the-art synonym substitution attacks. We find that their perturbations often do not preserve semantics, and 38{\%} introduce grammatical errors. Human surveys reveal that to successfully preserve semantics, we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences.With constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points.",
}

@inproceedings{mozes-etal-2021-contrasting,
    title = "Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification",
    author = "Mozes, Maximilian  and
      Bartolo, Max  and
      Stenetorp, Pontus  and
      Kleinberg, Bennett  and
      Griffin, Lewis",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.651",
    doi = "10.18653/v1/2021.emnlp-main.651",
    pages = "8258--8270",
    abstract = "Research shows that natural language processing models are generally considered to be vulnerable to adversarial attacks; but recent work has drawn attention to the issue of validating these adversarial inputs against certain criteria (e.g., the preservation of semantics and grammaticality). Enforcing constraints to uphold such criteria may render attacks unsuccessful, raising the question of whether valid attacks are actually feasible. In this work, we investigate this through the lens of human language ability. We report on crowdsourcing studies in which we task humans with iteratively modifying words in an input text, while receiving immediate model feedback, with the aim of causing a sentiment classification model to misclassify the example. Our findings suggest that humans are capable of generating a substantial amount of adversarial examples using semantics-preserving word substitutions. We analyze how human-generated adversarial examples compare to the recently proposed TextFooler, Genetic, BAE and SememePSO attack algorithms on the dimensions naturalness, preservation of sentiment, grammaticality and substitution rate. Our findings suggest that human-generated adversarial examples are not more able than the best algorithms to generate natural-reading, sentiment-preserving examples, though they do so by being much more computationally efficient.",
}

@inproceedings{mozes-etal-2021-frequency,
    title = "Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples",
    author = "Mozes, Maximilian  and
      Stenetorp, Pontus  and
      Kleinberg, Bennett  and
      Griffin, Lewis",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.13",
    doi = "10.18653/v1/2021.eacl-main.13",
    pages = "171--186",
    abstract = "Recent efforts have shown that neural text processing models are vulnerable to adversarial examples, but the nature of these examples is poorly understood. In this work, we show that adversarial attacks against CNN, LSTM and Transformer-based classification models perform word substitutions that are identifiable through frequency differences between replaced words and their corresponding substitutions. Based on these findings, we propose frequency-guided word substitutions (FGWS), a simple algorithm exploiting the frequency properties of adversarial word substitutions for the detection of adversarial examples. FGWS achieves strong performance by accurately detecting adversarial examples on the SST-2 and IMDb sentiment datasets, with F1 detection scores of up to 91.4{\%} against RoBERTa-based classification models. We compare our approach against a recently proposed perturbation discrimination framework and show that we outperform it by up to 13.0{\%} F1.",
}


@article{mozes_identifying_2022,
	title = {Identifying {Human} {Strategies} for {Generating} {Word}-{Level} {Adversarial} {Examples}},
	journal = {Findings of EMNLP 2022},
	author = {Mozes, Maximilian and Kleinberg, Bennett and Griffin, Lewis},
	year = {2022},
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?ðŸ¦œ},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@article{garg2018word,
  title={Word embeddings quantify 100 years of gender and ethnic stereotypes},
  author={Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={16},
  pages={E3635--E3644},
  year={2018},
  publisher={National Acad Sciences}
}

@article{argyle2022out,
  title={Out of One, Many: Using Language Models to Simulate Human Samples},
  author={Argyle, Lisa P and Busby, Ethan C and Fulda, Nancy and Gubler, Joshua and Rytting, Christopher and Wingate, David},
  journal={arXiv preprint arXiv:2209.06899},
  year={2022}
}

@article{jin2022make,
  title={When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment},
  author={Jin, Zhijing and Levine, Sydney and Gonzalez, Fernando and Kamal, Ojasv and Sap, Maarten and Sachan, Mrinmaya and Mihalcea, Rada and Tenenbaum, Josh and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:2210.01478},
  year={2022}
}

@article{sen2019total,
  title={A total error framework for digital traces of humans},
  author={Sen, Indira and Floeck, Fabian and Weller, Katrin and Weiss, Bernd and Wagner, Claudia},
  journal={arXiv preprint arXiv:1907.08228},
  year={2019}
}

@inproceedings{mitchell2019model,
  title={Model cards for model reporting},
  author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={220--229},
  year={2019}
}

@article{oswald2018algorithmic,
  title={Algorithmic risk assessment policing models: lessons from the Durham HART model and â€˜Experimentalâ€™proportionality},
  author={Oswald, Marion and Grace, Jamie and Urwin, Sheena and Barnes, Geoffrey C},
  journal={Information \& Communications Technology Law},
  volume={27},
  number={2},
  pages={223--250},
  year={2018},
  publisher={Taylor \& Francis}
}

@article{tversky1983extensional,
  title={Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment.},
  author={Tversky, Amos and Kahneman, Daniel},
  journal={Psychological review},
  volume={90},
  number={4},
  pages={293},
  year={1983},
  publisher={American Psychological Association}
}

@article{frederick2005cognitive,
  title={Cognitive reflection and decision making},
  author={Frederick, Shane},
  journal={Journal of Economic perspectives},
  volume={19},
  number={4},
  pages={25--42},
  year={2005}
}

@article{huang2022folk,
  title={Folk Theories of Online Dating: Exploring Peopleâ€™s Beliefs About the Online Dating Process and Online Dating Algorithms},
  author={Huang, Sabrina Angela and Hancock, Jeffrey and Tong, Stephanie Tom},
  journal={Social Media+ Society},
  volume={8},
  number={2},
  pages={20563051221089561},
  year={2022},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{devito2018people,
  title={How people form folk theories of social media feeds and what it means for how we study self-presentation},
  author={DeVito, Michael A and Birnholtz, Jeremy and Hancock, Jeffery T and French, Megan and Liu, Sunny},
  booktitle={Proceedings of the 2018 CHI conference on human factors in computing systems},
  pages={1--12},
  year={2018}
}

@inproceedings{devito2018algorithm,
  title={The algorithm and the user: How can hci use lay understandings of algorithmic systems?},
  author={DeVito, Michael A and Hancock, Jeffrey T and French, Megan and Birnholtz, Jeremy and Antin, Judd and Karahalios, Karrie and Tong, Stephanie and Shklovski, Irina},
  booktitle={Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
  pages={1--6},
  year={2018}
}

@article{jin2022make,
  title={When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment},
  author={Jin, Zhijing and Levine, Sydney and Gonzalez, Fernando and Kamal, Ojasv and Sap, Maarten and Sachan, Mrinmaya and Mihalcea, Rada and Tenenbaum, Josh and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:2210.01478},
  year={2022}
}

@article{sap2022neural,
  title={Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs},
  author={Sap, Maarten and LeBras, Ronan and Fried, Daniel and Choi, Yejin},
  journal={arXiv preprint arXiv:2210.13312},
  year={2022}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{goldstein2022shared,
  title={Shared computational principles for language processing in humans and deep language models},
  author={Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A and Feder, Amir and Emanuel, Dotan and Cohen, Alon and others},
  journal={Nature neuroscience},
  volume={25},
  number={3},
  pages={369--380},
  year={2022},
  publisher={Nature Publishing Group}
}

@inproceedings{bender2020climbing,
  title={Climbing towards NLU: On meaning, form, and understanding in the age of data},
  author={Bender, Emily M and Koller, Alexander},
  booktitle={Proceedings of the 58th annual meeting of the association for computational linguistics},
  pages={5185--5198},
  year={2020}
}

@article{henderson2022reproducible,
  title={A reproducible systematic map of research on the illusory truth effect},
  author={Henderson, Emma L and Westwood, Samuel J and Simons, Daniel J},
  journal={Psychonomic Bulletin \& Review},
  volume={29},
  number={3},
  pages={1065--1088},
  year={2022},
  publisher={Springer}
}

@inproceedings{lucy2021gender,
  title={Gender and representation bias in GPT-3 generated stories},
  author={Lucy, Li and Bamman, David},
  booktitle={Proceedings of the Third Workshop on Narrative Understanding},
  pages={48--55},
  year={2021}
}

@inproceedings{abid2021persistent,
  title={Persistent anti-muslim bias in large language models},
  author={Abid, Abubakar and Farooqi, Maheen and Zou, James},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={298--306},
  year={2021}
}

@article{rottweiler2021measuring,
  title={Measuring Individualsâ€™ Misogynistic Attitudes: Development and Validation of the Misogyny Scale},
  author={Rottweiler, Bettina and Gill, Paul},
  year={2021},
  publisher={PsyArXiv}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{van_der_vegt_multi-year_2022,
	title = {A multi-year study on insights into emotional coping during the pandemic},
	author = {van der Vegt, Isabelle and Mozes, Maximilian and Kleinberg, Bennett},
	year = {2022},
}

@inproceedings{griffin-etal-2023-large,
    title = "Large Language Models respond to Influence like Humans",
    author = "Griffin, Lewis  and
      Kleinberg, Bennett  and
      Mozes, Maximilian  and
      Mai, Kimberly  and
      Vau, Maria Do Mar  and
      Caldwell, Matthew  and
      Mavor-Parker, Augustine",
    editor = "Chawla, Kushal  and
      Shi, Weiyan",
    booktitle = "Proceedings of the First Workshop on Social Influence in Conversations (SICon 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.sicon-1.3",
    doi = "10.18653/v1/2023.sicon-1.3",
    pages = "15--24",
    abstract = "Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement boosts a later truthfulness test rating. Analysis of newly collected data from human and LLM-simulated subjects (1000 of each) showed the same pattern of effects in both populations; although with greater per statement variability for the LLM. The second study concerns a specific mode of influence {--} populist framing of news to increase its persuasion and political mobilization. Newly collected data from simulated subjects was compared to previously published data from a 15 country experiment on 7286 human participants. Several effects from the human study were replicated by the simulated study, including ones that surprised the authors of the human study by contradicting their theoretical expectations; but some significant relationships found in human data were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.",
}


@misc{kaddour_challenges_2023,
	title = {Challenges and {Applications} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.10169},
	abstract = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.},
	urldate = {2024-08-25},
	publisher = {arXiv},
	author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
	month = jul,
	year = {2023},
	note = {arXiv:2307.10169 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/BIP6CWGX/2307.html:text/html},
}


@misc{peereboom_cognitive_2024,
	title = {Cognitive phantoms in {LLMs} through the lens of latent variables},
	url = {http://arxiv.org/abs/2409.15324},
	abstract = {Large language models (LLMs) increasingly reach real-world applications, necessitating a better understanding of their behaviour. Their size and complexity complicate traditional assessment methods, causing the emergence of alternative approaches inspired by the field of psychology. Recent studies administering psychometric questionnaires to LLMs report human-like traits in LLMs, potentially influencing LLM behaviour. However, this approach suffers from a validity problem: it presupposes that these traits exist in LLMs and that they are measurable with tools designed for humans. Typical procedures rarely acknowledge the validity problem in LLMs, comparing and interpreting average LLM scores. This study investigates this problem by comparing latent structures of personality between humans and three LLMs using two validated personality questionnaires. Findings suggest that questionnaires designed for humans do not validly measure similar constructs in LLMs, and that these constructs may not exist in LLMs at all, highlighting the need for psychometric analyses of LLM responses to avoid chasing cognitive phantoms. Keywords: large language models, psychometrics, machine behaviour, latent variable modeling, validity},
	urldate = {2024-09-26},
	publisher = {arXiv},
	author = {Peereboom, Sanne and Schwabe, Inga and Kleinberg, Bennett},
	month = sep,
	year = {2024},
	note = {arXiv:2409.15324 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/N6Q6IJSS/2409.html:text/html},
}

@article{watson1913,
	title = {Psychology as the behaviorist views it},
	author = {Watson, John B.},
	year = {1913},
	month = {03},
	date = {1913-03},
	journal = {Psychological Review},
	pages = {158--177},
	volume = {20},
	number = {2},
	doi = {10.1037/h0074428},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0074428},
	langid = {en}
}

@article{skinner1935,
	title = {Two Types of Conditioned Reflex and a Pseudo Type},
	author = {Skinner, B. F.},
	year = {1935},
	month = {01},
	date = {1935-01},
	journal = {The Journal of General Psychology},
	pages = {66--77},
	volume = {12},
	number = {1},
	doi = {10.1080/00221309.1935.9920088},
	url = {https://www.tandfonline.com/doi/full/10.1080/00221309.1935.9920088},
	langid = {en}
}

@article{yerkes1909,
	title = {The method of Pawlow in animal psychology},
	author = {Yerkes, Robert M. and Morgulis, Sergius},
	year = {1909},
	month = {08},
	date = {1909-08},
	journal = {Psychological Bulletin},
	pages = {257--273},
	volume = {6},
	number = {8},
	doi = {10.1037/h0070886},
	url = {https://doi.apa.org/doi/10.1037/h0070886},
	langid = {en}
}

@article{jackson1986,
	title = {What Mary Didn't Know},
	author = {Jackson, Frank},
	year = {1986},
	month = {05},
	date = {1986-05},
	journal = {The Journal of Philosophy},
	pages = {291},
	volume = {83},
	number = {5},
	doi = {10.2307/2026143},
	url = {http://www.pdcnet.org/oom/service?url_ver=Z39.88-2004&rft_val_fmt=&rft.imuse_id=jphil_1986_0083_0005_0291_0295&svc_id=info:www.pdcnet.org/collection}
}

@inbook{chomsky1980,
	title = {4. A Review of B. F. Skinner{\textquoteright}s Verbal Behavior},
	author = {Chomsky, Noam},
	editor = {Block, Ned},
	year = {1980},
	month = {01},
	date = {1980-01-31},
	publisher = {Harvard University Press},
	doi = {10.4159/harvard.9780674594623.c6},
	url = {https://www.degruyter.com/document/doi/10.4159/harvard.9780674594623.c6/html},
	note = {DOI: 10.4159/harvard.9780674594623.c6},
	address = {Cambridge, MA and London, England}
}

@article{peereboom,
	title = {Cognitive phantoms in LLMs through the lens of latent variables},
	author = {Peereboom, Sanne and Schwabe, Inga and Kleinberg, Bennett}
}

@article{katz2022,
	title = {The Dark Side of Humanity Scale: A reconstruction of the Dark Tetrad constructs},
	author = {Katz, Louise and Harvey, Caroline and Baker, Ian S. and Howard, Chris},
	year = {2022},
	month = {02},
	date = {2022-02},
	journal = {Acta Psychologica},
	pages = {103461},
	volume = {222},
	doi = {10.1016/j.actpsy.2021.103461},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0001691821002110},
	langid = {en}
}

@article{templeton2024,
	title = {Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
	author = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
	year = {2024},
	date = {2024},
	journal = {Transformer Circuits Thread},
	url = {https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@article{bricken2023,
	title = {Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
	author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
	year = {2023},
	date = {2023},
	journal = {Transformer Circuits Thread}
}

@article{hofmann2024,
	title = {AI generates covertly racist decisions about people based on their dialect},
	author = {Hofmann, Valentin and Kalluri, Pratyusha Ria and Jurafsky, Dan and King, Sharese},
	year = {2024},
	month = {09},
	date = {2024-09-05},
	journal = {Nature},
	pages = {147--154},
	volume = {633},
	number = {8028},
	doi = {10.1038/s41586-024-07856-5},
	url = {https://www.nature.com/articles/s41586-024-07856-5},
	langid = {en}
}

@article{hackenburg2024,
	title = {Evaluating the persuasive influence of political microtargeting with large language models},
	author = {Hackenburg, Kobi and Margetts, Helen},
	year = {2024},
	month = {06},
	date = {2024-06-11},
	journal = {Proceedings of the National Academy of Sciences},
	pages = {e2403116121},
	volume = {121},
	number = {24},
	doi = {10.1073/pnas.2403116121},
	url = {https://pnas.org/doi/10.1073/pnas.2403116121},
	langid = {en}
}

@article{urbina2022,
	title = {Dual use of artificial-intelligence-powered drug discovery},
	author = {Urbina, Fabio and Lentzos, Filippa and Invernizzi, {CÃ©dric} and Ekins, Sean},
	year = {2022},
	month = {03},
	date = {2022-03-07},
	journal = {Nature Machine Intelligence},
	pages = {189--191},
	volume = {4},
	number = {3},
	doi = {10.1038/s42256-022-00465-9},
	url = {https://www.nature.com/articles/s42256-022-00465-9},
	langid = {en}
}

@article{costello2024,
	title = {Durably reducing conspiracy beliefs through dialogues with AI},
	author = {Costello, Thomas H. and Pennycook, Gordon and Rand, David G.},
	year = {2024},
	month = {09},
	date = {2024-09-13},
	journal = {Science},
	pages = {eadq1814},
	volume = {385},
	number = {6714},
	doi = {10.1126/science.adq1814},
	url = {https://www.science.org/doi/10.1126/science.adq1814},
	langid = {en}
}

@article{jakesch2023,
	title = {Human heuristics for AI-generated language are flawed},
	author = {Jakesch, Maurice and Hancock, Jeffrey T. and Naaman, Mor},
	year = {2023},
	month = {03},
	date = {2023-03-14},
	journal = {Proceedings of the National Academy of Sciences},
	pages = {e2208839120},
	volume = {120},
	number = {11},
	doi = {10.1073/pnas.2208839120},
	url = {https://pnas.org/doi/10.1073/pnas.2208839120},
	langid = {en}
}

@article{millar2022,
	title = {Trends in the Use of Promotional Language (Hype) in Abstracts of Successful National Institutes of Health Grant Applications, 1985-2020},
	author = {Millar, Neil and Batalo, Bojan and Budgell, Brian},
	year = {2022},
	month = {08},
	date = {2022-08-25},
	journal = {JAMA Network Open},
	pages = {e2228676},
	volume = {5},
	number = {8},
	doi = {10.1001/jamanetworkopen.2022.28676},
	url = {https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2795635},
	langid = {en}
}

@inproceedings{pennington2014,
	title = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	date = {2014},
	publisher = {Association for Computational Linguistics},
	pages = {1532--1543},
	doi = {10.3115/v1/D14-1162},
	url = {http://aclweb.org/anthology/D14-1162},
	address = {Doha, Qatar},
	langid = {en}
}

@article{mikolov2013,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	year = {2013},
	month = {10},
	date = {2013-10-16},
	journal = {arXiv:1310.4546 [cs, stat]},
	url = {http://arxiv.org/abs/1310.4546},
	note = {arXiv: 1310.4546}
}

@article{harris1954,
	title = {Distributional hypothesis},
	author = {Harris, Zellig},
	year = {1954},
	date = {1954},
	journal = {Word World},
	pages = {146{\textendash}162},
	volume = {10},
	number = {23}
}

@inproceedings{devlin2019,
	title = {NAACL-HLT 2019},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	month = {06},
	date = {2019-06},
	publisher = {Association for Computational Linguistics},
	pages = {4171{\textendash}4186},
	doi = {10.18653/v1/N19-1423},
	url = {https://www.aclweb.org/anthology/N19-1423},
	address = {Minneapolis, Minnesota}
}

@article{vandervegt2023,
	title = {A multi-modal panel dataset to understand the psychological impact of the pandemic},
	author = {Van Der Vegt, Isabelle and Kleinberg, Bennett},
	year = {2023},
	month = {08},
	date = {2023-08-11},
	journal = {Scientific Data},
	pages = {537},
	volume = {10},
	number = {1},
	doi = {10.1038/s41597-023-02438-y},
	url = {https://www.nature.com/articles/s41597-023-02438-y},
	langid = {en}
}

@article{grootendorst2022,
	title = {BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
	author = {Grootendorst, Maarten},
	year = {2022},
	date = {2022},
	journal = {arXiv preprint arXiv:2203.05794}
}

@techreport{boyd2022,
	title = {The development and psychometric properties of LIWC-22},
	author = {Boyd, Ryan L. and Ashokkumar, A. and Seraj, S. and Pennebaker, J. W.},
	year = {2022},
	date = {2022},
	url = {https://www.liwc.app/},
	address = {Austin, TX}
}

@inproceedings{reimers2019,
	title = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	author = {Reimers, Nils and Gurevych, Iryna},
	year = {2019},
	date = {2019},
	publisher = {Association for Computational Linguistics},
	pages = {3980--3990},
	doi = {10.18653/v1/D19-1410},
	url = {https://www.aclweb.org/anthology/D19-1410},
	address = {Hong Kong, China},
	langid = {en}
}

@article{kleinberg2021,
	title = {How humans impair automated deception detection performance},
	author = {Kleinberg, Bennett and Verschuere, Bruno},
	year = {2021},
	month = {02},
	date = {2021-02},
	journal = {Acta Psychologica},
	pages = {103250},
	volume = {213},
	doi = {10.1016/j.actpsy.2020.103250},
	url = {http://dx.doi.org/10.1016/j.actpsy.2020.103250},
	langid = {en}
}
