---
title: 'Practical I (solutions)'
subtitle: Bennett Kleinberg
date: 'Statistical Natural Language Processing in R'
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: show
  html_notebook:
    theme: united
    toc: yes
    code_folding: show
  pdf_document:
    toc: yes
    code_folding: show
---

## Aims of this practical

- Getting started with handling textual data in R
- Basic steps in data cleaning
- Calculating text metrics
- Replicating Zipf's Law


## Task 1: Corpus operations

Before we work with text data in a more advanced manner, we will first start by using datasets that are contained in R and then move to loading external data sets on which we conduct text-based operations. 

R - and `quanteda` specifically - contain numerous "built-in" datasets. You can find these under [https://quanteda.io/reference/index.html#section-data](https://quanteda.io/reference/index.html#section-data).

By loading the `quanteda` package, these datasets are available in your workspace and can be accessed.

```{r}
library(quanteda)
library(data.table)

# we use the dataset of inaugural speeches by US presidents as the first example
data_corpus_inaugural
```


To access the individual texts, you can simply index the object: 


```{r}
us_speeches = data_corpus_inaugural
us_speeches[1]

```

Note that this corpus object also contains `dovcars` (document-level variables). These are essential for later analyses and classification tasks. We can see what these variables are as follows:

```{r}
docvars(us_speeches)
```

See more on document variables - including how you can assign them (useful for later steps) here: [https://tutorials.quanteda.io/basic-operations/corpus/docvars/](https://tutorials.quanteda.io/basic-operations/corpus/docvars/). For now, it suffices to access the `docvars` in the usual form:

```{r}
us_speeches$Year
```

Lastly, each document's name can be accessed as:

```{r}
docnames(us_speeches)
```


### Exercise 1.1 

**Which speech has the highest number of characters per word? And which one the lowest?**

_Hint: try to work with the native data.frame structure or with a data.table. This will require a conversion from the corpus object._

```{r}
# Option 1: use the native data.frame structure (preferably: data.table for significantly faster processing for larger corpora - see below)

##  convert the corpus to a data.frame
us_corpus_df = convert(us_speeches, to = 'data.frame')
names(us_corpus_df)

## add columns to the data.frame
us_corpus_df$nchars = nchar(us_corpus_df$text)
us_corpus_df$ntoks = ntoken(quanteda::tokens(us_corpus_df$text, what = 'word')) #note the forcing to use quanteda's tokens function; this is due to other packages also containing this function

## create the variable of characters per word
us_corpus_df$cpw = us_corpus_df$nchars/us_corpus_df$ntoks

## lastly: find the highest and lowest
us_corpus_df[which.max(us_corpus_df$cpw), ]
us_corpus_df[which.min(us_corpus_df$cpw), ]


# Option 2: using data.table for the steps above 
## The syntax is somewhat different but strongly recommend for faster processing.

us_corpus_dt = setDT(convert(us_speeches, to = 'data.frame')) # The trick here is to go via a df first and then set the data.table

us_corpus_dt[, cpw := nchar(text)/ntoken(quanteda::tokens(text))]

us_corpus_dt[which.max(cpw), ]
us_corpus_dt[which.min(cpw), ]
```


### Exercise 1.2 

**Which speech contained the most punctuation?**

```{r}
# Using data.table
## Using the tokens_select() function and defining a punctuation regex to keep the selection of all punctuation, then count what is left over
us_corpus_dt[, n_punct := ntoken(tokens_select(quanteda::tokens(text)
                                               , pattern = "[[:punct:]]"
                                               , valuetype = "regex"
                                               , selection = 'keep'))]


## We may want to standardise this by the length of the speech
us_corpus_dt[, prop_punct := n_punct/ntoken(quanteda::tokens(text))]

us_corpus_dt[which.max(n_punct), ]
us_corpus_dt[which.max(prop_punct), ]

```

### Exercise 1.3 

**How has the average sentence length changes over time?**

```{r}
# calculate words per sentence
us_corpus_dt[, ntoks := ntoken(quanteda::tokens(text, what = 'word'))]
us_corpus_dt[, nsent := ntoken(quanteda::tokens(text, what = 'sentence'))]

us_corpus_dt[, wps := ntoks/nsent]

# order by year
us_c_dt_year = us_corpus_dt[order(Year), ]

# plot
plot(us_c_dt_year$Year
     , us_c_dt_year$wps
     , xlab='Year'
     , ylab='WPS')
```



## Task 2: First steps with real datasets

Use the data of statements on truthful and deceptive weekend plans that was the basis for [this paper](https://www.sciencedirect.com/science/article/pii/S0001691820305746). You can find the raw textual data on the OSF: [https://osf.io/rtq9y](https://osf.io/rtq9y). 

The participants were asked to either tell the truth about their plans for the upcoming weekend or were assigned an activity from someone else and had to lie about it (i.e., fabricate a story).

Each participant was asked two provide two statements (1. Please write about your weekend plans in as much detail as possible.; 2. Which information could prove that you are telling the truth?). Focus on the first question (called `q1` in the dataset).

The variable `outcome_class` is either `t` (truthful) or `d` (deceptive).

### Exercise 2.1 

**What is the effect size (Cohen's d) for the difference in words per sentence between truthful and deceptive statements?**

```{r}
# loading the data (here: all in the data.table flow)

ex_data = fread('/Users/bennettkleinberg/GitHub/snlp/data/sign_events_data_statements.csv')

names(ex_data)

# adding the columns
ex_data[, ntoks := ntoken(quanteda::tokens(q1, what = 'word'))]
ex_data[, nsent := ntoken(quanteda::tokens(q1, what = 'sentence'))]

ex_data[, wps := ntoks/nsent]


# obtaining the effect size for the wps ~ outcome_class
## Here using the effectsize package
library(effectsize)
cohens_d(data = ex_data
         , wps ~ outcome_class, ci = .95)

## Descriptives of that effect with data.table
ex_data[, .('M' = mean(wps)
            , 'SD' = sd(wps))
        , by = .(outcome_class)]
```

## Task 3: Replicating Zipf's Law

A curious "law" in corpus linguistics is Zipf's Law ([YouTube here](https://www.youtube.com/watch?v=fCn8zs912OE)).

Zipf's Law describes the relationship between the frequency of words in a language and their rank in a frequency-sorted list: the frequency of any word is inversely proportional to its rank in the frequency table.


Key aspects of Zipf's Law:

- Word frequency distribution: In a large enough collection of texts, the most common word occurs about twice as often as the second most frequent word, three times as often as the third most frequent word, etc.
- Mathematical formulation: The law can be expressed as $f(w) \approx \frac{1}{r}$, where $f(w)$ is the frequency of word $w$ and r is the rank of the word.
- Universality: This distribution is observed across various languages, including children's speech and specialized vocabularies.


### Exercise 3.1

The dataset we will use for this exercise stems from work a [paper on analysing narrative shapes in YouTube vlog transcripts](https://aclanthology.org/D18-1394/). In that paper, the video transcripts of 30k vlogs were analysed. The dataset can be loaded as follows:

```{r}
load('/Users/bennettkleinberg/GitHub/snlp/data/vlogs_corpus.RData')

vlogs_corpus
```

**Does Zipf's Law apply to a corpus of YouTube vlog transcripts?**

_Hint: you will need to obtain the most common words for this analysis from that corpus. Have a look at the [`topfeatures()` function](https://www.rdocumentation.org/packages/quanteda/versions/1.3.13/topics/topfeatures). Here, put your tokenised object into a `dfm` (we will learn more about the dfm in the next part)._

```{r}
# create a corpus object
c_vlogs_corpus = corpus(vlogs_corpus)

# note how you automatically retain the document-level variables
docvars(c_vlogs_corpus)

# tokenise the corpus
toks_c = quanteda::tokens(c_vlogs_corpus)

# remember that Zipf's Law states that the frequency of a word is inversely proportional to that word's rank
## we first obtain the most common words

top_100 = topfeatures(dfm(toks_c), n = 100)

## we can then create "predictions" by Zipf's Law
zipf_pred = 1/1:100

## the prediction (if Zipf's Law would be at play) would be
plot(x = 1:100
     , y = zipf_pred
     , xlab = 'Observed rank'
     , ylab = 'Pred. acc. to Zipf'
     , type='l'
     , col = 'blue')


## on our data
plot(x = 1:100
     , y = top_100
     , xlab='Observed rank'
     , ylab='Frequency'
     , type='l'
     , col = 'red')

```

### Exercise 3.2 

**How do the word frequency ranks in the vlogs corpus deviate from Google's 1 Trillion Word Corpus frequency ranks?**

You can find a ranked list of word frequencies from from Google's Trillion Word Corpus at: [https://github.com/first20hours/google-10000-english](https://github.com/first20hours/google-10000-english). It is also provided in the `data` directory of this repo (`./data/google_10k_list.txt`). These data are already in ranked order; the file does not contain a header (so set: `header=F`).

```{r}
# load the google frequencies
google_ranks = fread('/Users/bennettkleinberg/GitHub/snlp/data/google_10k_list.txt'
                      , header=F)

# assign rank variable
google_ranks[, rank := 1:.N]

# rename variable
names(google_ranks)[1] = 'word_google'

# select only top 100
google_top_100 = google_ranks[1:100, ]

# get top 100 from vlogs to data.table / data.frame
top_100_dt = setDT(data.frame(word_vlog = names(top_100)
                              , rank = 1:100))

# merge both
ranks_merged = merge(google_top_100, top_100_dt, by='rank')

```

---