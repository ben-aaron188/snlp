---
title: 'Practical III (solutions)'
subtitle: Bennett Kleinberg
date: 'Statistical Natural Language Processing in R'
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: show
  html_notebook:
    theme: united
    toc: yes
    code_folding: show
  pdf_document:
    toc: yes
    code_folding: show
---

## Aims of this practical

-   practicing sentiment analysis on a dataset of altright influencer vlogs
-   broadening sentiment analysis to emotion analysis
-   applying a dictionary to obtain hate speech measurements
-   working with your own dictionary

```{r}
library(data.table)
library(sentimentr)
library(quanteda)
```


## Task 1: Sentiment analysis in vlogs of altright influencers

For this task, we will use a dataset that we gathered a few years ago on YouTube's ["Creators for Change" initiative](https://www.youtube.com/creators/).

> YouTube Creators for Change is a new initiative dedicated to amplifying and multiplying the voices of role models who are tackling difficult social issues with their channels. From combating hate speech, to countering xenophobia and extremism, to simply making the case for greater tolerance and empathy toward others, these creators are helping generate positive social change with their global fan bases.

We have gathered data from the earliest creators for change as well as from channels considered as "toxic" as a counterpart and a reference sample of "normal" YouTube channels. The data used here are a stratified sample of the larger dataset (the larger dataset requires more memory and processing time but if you are keen on using it, you can find it at: [https://www.dropbox.com/scl/fi/ic96uiu92dqnyaxouzlzo/data_cfc_vlogs.RData?rlkey=psttsoodcucjt9sha1t3ce9w8&dl=0](https://www.dropbox.com/scl/fi/ic96uiu92dqnyaxouzlzo/data_cfc_vlogs.RData?rlkey=psttsoodcucjt9sha1t3ce9w8&dl=0).

Load the data as follows:

```{r}
load('/Users/bennettkleinberg/GitHub/snlp/data/data_cfc_vlogs_sampled.RData')

names(cfc_vlogs_sampled)
```

You can see how many vlogs there are in each class:

```{r}
# data.table
cfc_vlogs_sampled[, .N, .(class)]

# data.frame syntax
table(cfc_vlogs_sampled$class)
```

The corpus contains (763k tokens):

```{r}
sum(cfc_vlogs_sampled$nwords)
```



### Exercise 1.1

**Compare the sentiment between vlogs from the creators for change initiatives with the toxic comparison sample and the normal YouTube channels. What are the average sentiment values for the three classes?**

Note: ideally you want to conduct any statistical analysis on this dataset in a multilevel model since there is nesting of vlogs in vloggers but you can also ignore this for now since the focus is on the NLP techniques.

_Hint: the sentimentr approach relies on sentences and runs a sentence disambiguation each time it runs. You can speed the process up by creating the sentences first and then pass these on to the sentiment function._

```{r}
# create sentences
vlogs_sentences = sentimentr::get_sentences(cfc_vlogs_sampled$text)

# create a new object which contains the sentiment object from the sentimentr approach
cfc_sentiment = sentimentr::sentiment(text.var = vlogs_sentences
                                      , polarity_dt = lexicon::hash_sentiment_sentiword
                      , valence_shifters_dt = lexicon::hash_valence_shifters)


# The above provides us with the sentiment per sentence (here less applicable since most of the transcripts do not contain punctuation), so we aggregate the data by the next highest hierarchy: element_id.

cfc_sentiment_aggr = cfc_sentiment[, mean(sentiment), by = .(element_id)]

# we now combine these data with our main data that contains all meta variables
vlogs_sentiment = cbind(cfc_vlogs_sampled, cfc_sentiment_aggr)
names(vlogs_sentiment)[22] = 'sentiment'

# Further: to obtain the sentiment by class, we aggregate by "class"
# In data.table, you can aggregate with multiple aggregation functions as follows (and name them)
vlogs_sentiment[, .("M" = mean(sentiment)
                    , "SD" = sd(sentiment))
                , by = .(class)]
```


### Exercise 1.2

**Is the sentiment of the vlogs correlated to the view count of the vlogs?**

You can use the `view_count_corrected` variable which corrects for the duration the vlog was available.

```{r}
# we assume a non-parametric correlation is most adequate here since view counts rarely follow a normal distribution

cor.test(vlogs_sentiment$sentiment, vlogs_sentiment$view_count_corrected
         , method = 'spearman')


# if you wanted to assess whether that correlation is dependent on the class, you can incorporate this in a linear model (or a non-parametric version thereof)

lm_1 = lm(data = vlogs_sentiment
          , formula = view_count_corrected ~ sentiment*class)
summary(lm_1)

```


### Exercise 1.3

**Which vlog of the entire dataset has the most positive (negative) sentiment?**


```{r}
vlogs_sentiment[which.max(vlogs_sentiment$sentiment), ]

vlogs_sentiment[which.min(vlogs_sentiment$sentiment), ]
```


## Task 2: Emotion analysis

We can now broaden the sentiment analysis to emotion analysis. The mechanism is the same but instead of just measuring sentiment, we can now use dictionaries that allow us to measure range of emotions conveyed through the vocabulary.

### Exercise 2.1

**Conduct an emotion analysis by changing the dictionary that is used by the sentiment function of the `sentimentr` package. (This is wrapped in the `sentimentr::emotion` function.)**

Before you start with this, have a look at the `emotion` function --> `?emotion`

```{r}
# You can find a good emotion lexicon as follows:
lexicon::hash_nrc_emotions
```


```{r}
cfc_emotion = sentimentr::emotion(text.var = vlogs_sentences
                                  , emotion_dt = lexicon::hash_nrc_emotions
                                  , valence_shifters_dt = lexicon::hash_valence_shifters)

head(cfc_emotion)

# looking at the structure of the output, we see that we may need to aggregate on several levels: element_id, emotion_type and emotion (the value) or emotion_count
# Note: in this case, the data will not change since each document is one sentence

cfc_emotion_aggr = cfc_emotion[, mean(emotion), .(emotion_type, element_id)]
names(cfc_emotion_aggr)[3] = "emotion_value"

# we might want to reshape this to a wide format for combining it with the main data

library(tidyr)
cfc_emotion_aggr_wide = pivot_wider(cfc_emotion_aggr
                                    , id_cols = element_id
                                    , names_from = emotion_type
                                    , values_from = emotion_value)

cfc_emotion_aggr_wide
```



### Exercise 2.2

**What is the most dominant emotion for each of the three classes?**

Hint: you may want to reshape the data to a wide format first.

```{r}
# combine the wide emotion data from above with the main data
vlogs_emotion = cbind(cfc_vlogs_sampled, cfc_emotion_aggr_wide)

# There are many ways to do this. In the option below, we define the emotion columns and use them in the data.table ".SDcols" format
emotion_cols = names(vlogs_emotion)[22:37]

vlogs_emotion[, lapply(.SD, mean), .SDcols = emotion_cols, by = .(class)]
```



## Task 3: Measuring hate speech

Another dimension that has been research in the past decade is hate speech. One approach to measure hate speech is with existing databases of hateful terms or words.

### Exercise 3.1

An initiative to gather hateful content is [Hatebase](https://hatebase.org/) with their mission stated as follows:

> Hatebase was built to assist companies, government agencies, NGOs and research organizations moderate online conversations and potentially use hate speech as a predictor for regional violence. (Language-based classification, or symbolization, is one of a handful of quantifiable steps toward genocide.)

We have prepared the hatebase data on ethnicity in a dictionary YAML file under `./data/hatebase_ethnicity_lexicon.yml`. For more info on that approach, see REF

**Run the hatebase ethnicity dictionary on the vlogs data and identify which vlogger contains the most hate speech according to this measurement approach.**

```{r}
# load the dictionary
library(quanteda)
hatebase_dict = dictionary(file = "/Users/bennettkleinberg/GitHub/snlp/data/hatebase_ethnicity_lexicon.yml")

# build a dfm and then look up the dictionary terms
c1 = corpus(cfc_vlogs_sampled$text
            , docvars = data.frame(class = cfc_vlogs_sampled$class))

c_tok = tokens(c1
               , remove_punct = T)

c_tok = tokens_select(c_tok
                      , pattern = stopwords("en")
                      , selection = "remove")

c_dfm = dfm(c_tok
            , tolower = T)

dict_match = dfm_lookup(c_dfm
                 , dictionary = hatebase_dict
                 , valuetype = 'glob')

dict_match
```


### Exercise 3.2

**What is the effect size (Cohen's d) of the difference in ethnicity slurs between creators for change and toxic vloggers as measured by the Hatebase dictionary you imported above?**

```{r}
df_dict_match = convert(dict_match, to='data.frame')
df_dict_match$class = dict_match$class

# work with a data.table
dt_dict_match = setDT(df_dict_match)

effectsize::cohens_d(data = dt_dict_match[class != 'nor', ]
                       , hb_ethnicity ~ class)


dt_dict_match[, .("M" = mean(hb_ethnicity)
                    , "SD" = sd(hb_ethnicity))
                , by = .(class)]

```


## Task 4: Your own dictionary

In this task, you can create your own dictionary. The key here is that you adhere to the structure that is expected to create a YAML file which can then be stored as a dictionary and imported so you can make a query with the DFM object.

### Exercise 4.1

**Create your own dictionary with at least three categories and apply it to (a selection of) the vlogs corpus you already loaded above.**


```{r}
my_dict = quanteda::dictionary(list(vlogger_speak = c("yada", "like", "oh", "kind of",  "yeah")
                                      , immigration_theme = c("foreign" , "immigration" , "immigrant" , "border", "illegal")
                                      , western_themes = c("white", "christianity", "bible", "christian" , "western", "god")
                                      )
                                 )

dict_match_2 = dfm_lookup(c_dfm
                 , dictionary = my_dict
                 , valuetype = 'glob')

dict_match_2


```


------------------------------------------------------------------------
